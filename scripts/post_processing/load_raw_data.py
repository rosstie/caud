#!/usr/bin/env python3
"""
Load raw simulation data from parquet files generated by convert_zarr_to_parquet.py.

This script provides functions to load and analyze raw simulation data, similar to
the aggregated data loading in simple_analysis.py but without aggregation.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import logging
import json

# Set up logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger("load_raw_data")


def load_raw_data(raw_data_dir):
    """
    Load the raw data from parquet files and perform any necessary preprocessing.

    Parameters:
    -----------
    raw_data_dir : str
        Path to the directory containing the raw data parquet files

    Returns:
    --------
    dict
        Dictionary containing the loaded data for each parameter set:
        - data: Combined DataFrame containing all parameter sets
        - param_sets: Dictionary mapping parameter set names to their DataFrames
        - parameters: DataFrame with parameter values for each parameter set
    """
    # Convert to Path object for easier manipulation
    data_dir = Path(raw_data_dir)

    if not data_dir.exists():
        logger.error(f"Directory '{data_dir}' does not exist.")
        return None

    # Get all parquet files
    parquet_files = list(data_dir.glob("*.parquet"))

    if not parquet_files:
        logger.error(f"No parquet files found in {data_dir}")
        return None

    logger.info(f"Found {len(parquet_files)} parquet files in {data_dir}")

    # Dictionary to store DataFrames for each parameter set
    param_sets = {}

    # List to store parameter values
    param_info = []

    # Load each parquet file
    for parquet_file in parquet_files:
        param_name = parquet_file.stem
        logger.info(f"Loading parameter set: {param_name}")

        try:
            # Load the parquet file with explicit use of pyarrow engine to preserve data types
            df = pd.read_parquet(parquet_file, engine="pyarrow")

            # Convert any JSON string columns back to Python objects
            for col in df.columns:
                if df[col].dtype == "object":
                    # Check the first few non-null values to detect JSON strings
                    sample = df[col].dropna().head(5)

                    # If all sampled values are strings that look like JSON
                    if (
                        len(sample) > 0
                        and all(isinstance(x, str) for x in sample)
                        and all(
                            (x.startswith("{") and x.endswith("}"))
                            or (x.startswith("[") and x.endswith("]"))
                            for x in sample
                        )
                    ):
                        try:
                            # Convert JSON strings to Python objects
                            df[col] = df[col].apply(
                                lambda x: json.loads(x)
                                if isinstance(x, str)
                                and (
                                    (x.startswith("{") and x.endswith("}"))
                                    or (x.startswith("[") and x.endswith("]"))
                                )
                                else x
                            )
                            logger.info(
                                f"Converted column {col} from JSON strings to Python objects"
                            )
                        except (ValueError, json.JSONDecodeError):
                            logger.warning(
                                f"Failed to convert column {col} from JSON strings"
                            )

                    # Try to convert string numeric values to actual numeric values
                    if all(
                        isinstance(x, str) and x.replace(".", "", 1).isdigit()
                        for x in sample
                        if isinstance(x, str) and x
                    ):
                        try:
                            df[col] = pd.to_numeric(df[col])
                            logger.info(f"Converted string column {col} to numeric")
                        except (ValueError, TypeError):
                            logger.warning(
                                f"Failed to convert string column {col} to numeric"
                            )

            # Store in the dictionary
            param_sets[param_name] = df

            # Extract parameter values from the first row (they should be the same for all rows in a param set)
            param_data = {"param_dir": param_name}

            # Extract parameter values - handle both prefixed and non-prefixed column names
            param_prefixes = ["meta.param_", "param_"]

            # Get all parameter columns
            param_cols = []
            for prefix in param_prefixes:
                param_cols.extend([col for col in df.columns if col.startswith(prefix)])

            if param_cols and len(df) > 0:
                for col in param_cols:
                    # Remove prefix to get parameter name
                    for prefix in param_prefixes:
                        if col.startswith(prefix):
                            param_key = col.replace(prefix, "")
                            break
                    param_data[param_key] = df[col].iloc[0]

                param_info.append(param_data)

            logger.info(
                f"  Loaded {param_name}: {df.shape[0]} rows, {df.shape[1]} columns"
            )

        except Exception as e:
            logger.error(f"Error loading {param_name}: {e}")

    # Convert parameter data to DataFrame
    param_df = pd.DataFrame(param_info)

    # Combine all parameter sets into a single DataFrame
    combined_df = pd.concat(param_sets.values(), ignore_index=True)

    logger.info(f"Combined data shape: {combined_df.shape}")

    return {"data": combined_df, "param_sets": param_sets, "parameters": param_df}


def filter_data(
    data, k_values=None, network_densities=None, agent_groups=None, run_ids=None
):
    """
    Filter the data based on specified parameters.

    Parameters:
    -----------
    data : dict
        Data dictionary from load_raw_data
    k_values : list or None
        List of K values to include
    network_densities : list or None
        List of network density values to include
    agent_groups : list or None
        List of agent group values to include
    run_ids : list or None
        List of run IDs to include

    Returns:
    --------
    pandas.DataFrame
        Filtered DataFrame
    """
    df = data["data"]

    # Apply filters - handling both prefixed and non-prefixed columns
    k_columns = ["meta.K_NK", "K_NK"]
    if k_values is not None:
        # Try to find the first valid column
        for col in k_columns:
            if col in df.columns:
                df = df[df[col].isin(k_values)]
                break

    density_columns = ["meta.p_er_network", "p_er_network"]
    if network_densities is not None:
        for col in density_columns:
            if col in df.columns:
                df = df[df[col].isin(network_densities)]
                break

    group_columns = ["meta.number_of_groups", "number_of_groups"]
    if agent_groups is not None:
        for col in group_columns:
            if col in df.columns:
                df = df[df[col].isin(agent_groups)]
                break

    if run_ids is not None:
        df = df[df["run_id"].isin(run_ids)]

    return df


def create_disaster_regime_category(df):
    """
    Create disaster regime categories based on input parameters.

    Parameters:
    -----------
    df : pandas.DataFrame
        DataFrame containing simulation data

    Returns:
    --------
    pandas.DataFrame
        DataFrame with added disaster regime categories
    """
    # Create a copy to avoid modifying the original
    result = df.copy()

    # Initialize columns with default values
    result["disaster_regime_prob"] = "Unknown"
    result["disaster_regime_impact"] = "Unknown"
    result["disaster_regime_skew"] = "Unknown"

    # Map disaster probability to categories - look for the column with the right prefix
    prob_prefixes = ["meta.param_", "param_"]
    prob_col = None
    for prefix in prob_prefixes:
        col = f"{prefix}disasterProbability"
        if col in result.columns:
            prob_col = col
            break

    if prob_col:
        result.loc[result[prob_col] == 0.01, "disaster_regime_prob"] = "Rare"
        result.loc[result[prob_col] == 0.04, "disaster_regime_prob"] = "Frequent"

    # Map disaster impact to categories
    impact_prefixes = ["meta.param_", "param_"]
    impact_col = None
    for prefix in impact_prefixes:
        col = f"{prefix}95th_percentile"
        if col in result.columns:
            impact_col = col
            break

    if impact_col:
        result.loc[result[impact_col] == 0.5, "disaster_regime_impact"] = "Mild"
        result.loc[result[impact_col] == 0.65, "disaster_regime_impact"] = "Strong"

    # Map disaster clustering to categories
    cluster_prefixes = ["meta.param_", "param_"]
    cluster_col = None
    for prefix in cluster_prefixes:
        col = f"{prefix}disasterClusteredness"
        if col in result.columns:
            cluster_col = col
            break

    if cluster_col:
        result.loc[result[cluster_col] == 0, "disaster_regime_skew"] = "Uniform"
        result.loc[result[cluster_col] == 0.5, "disaster_regime_skew"] = "Clustered"

    return result


def categorize_disaster_severity(
    df,
    freq_col=None,
    impact_col=None,
    use_median_split=True,
):
    """
    Categorize data based on observed disaster metrics.

    Parameters:
    -----------
    df : pandas.DataFrame
        Data to categorize
    freq_col : str
        Column name for disaster frequency measure (if None, tries to find it)
    impact_col : str
        Column name for disaster impact measure (if None, tries to find it)
    use_median_split : bool
        If True, use median split; if False, use tertiles

    Returns:
    --------
    pandas.DataFrame
        DataFrame with added disaster severity categories
    """
    # Make a copy to avoid modifying the original
    result = df.copy()

    # Find the appropriate columns if not provided
    if freq_col is None:
        freq_candidates = ["disaster.count", "count"]
        for col in freq_candidates:
            if col in df.columns:
                freq_col = col
                break

    if impact_col is None:
        impact_candidates = ["disaster.average_impact_rmsd", "average_impact_rmsd"]
        for col in impact_candidates:
            if col in df.columns:
                impact_col = col
                break

    # Return original DataFrame if we can't find the columns
    if freq_col is None or impact_col is None:
        logger.warning("Could not find disaster count or impact columns")
        return result

    # For disaster frequency
    if use_median_split:
        median_freq = result[freq_col].median()
        result["disaster_frequency"] = np.where(
            result[freq_col] <= median_freq, "Low", "High"
        )
    else:
        # Use tertiles
        result["disaster_frequency"] = "Medium"
        result.loc[
            result[freq_col] <= result[freq_col].quantile(0.33), "disaster_frequency"
        ] = "Low"
        result.loc[
            result[freq_col] >= result[freq_col].quantile(0.67), "disaster_frequency"
        ] = "High"

    # For disaster impact
    if use_median_split:
        median_impact = result[impact_col].median()
        result["disaster_impact"] = np.where(
            result[impact_col] <= median_impact, "Low", "High"
        )
    else:
        # Use tertiles
        result["disaster_impact"] = "Medium"
        result.loc[
            result[impact_col] <= result[impact_col].quantile(0.33), "disaster_impact"
        ] = "Low"
        result.loc[
            result[impact_col] >= result[impact_col].quantile(0.67), "disaster_impact"
        ] = "High"

    # Combined severity
    result["disaster_severity"] = (
        result["disaster_frequency"] + "_" + result["disaster_impact"]
    )

    return result


def normalize_dataframe(df, method="zscore", exclude_cols=None):
    """
    Normalize numerical columns in a dataframe using various methods.

    Parameters:
    -----------
    df : pandas.DataFrame
        DataFrame to normalize
    method : str, default='zscore'
        Normalization method to use:
        - 'zscore': Standardize to mean=0, std=1
        - 'minmax': Scale to range [0,1]
        - 'robust': Scale based on median and quantiles
        - 'log': Natural logarithm (adds small constant to avoid log(0))
    exclude_cols : list or None
        List of column names to exclude from normalization

    Returns:
    --------
    pandas.DataFrame
        Normalized DataFrame with original non-numeric and excluded columns
    """
    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler

    # Make a copy of the input DataFrame
    df_normalized = df.copy()

    # Default exclude_cols to empty list if None
    if exclude_cols is None:
        exclude_cols = []

    # Add parameter columns to exclude list (assuming they start with "meta.param_" or "param_")
    param_cols = []
    for prefix in ["meta.param_", "param_"]:
        param_cols.extend([col for col in df.columns if col.startswith(prefix)])
    exclude_cols = list(set(exclude_cols + param_cols))

    # Identify numeric columns to normalize (excluding specified columns)
    numeric_cols = df.select_dtypes(include=np.number).columns
    cols_to_normalize = [col for col in numeric_cols if col not in exclude_cols]

    if len(cols_to_normalize) == 0:
        logger.warning("No columns to normalize.")
        return df_normalized

    # Apply the specified normalization method
    if method == "zscore":
        scaler = StandardScaler()
        df_normalized[cols_to_normalize] = scaler.fit_transform(df[cols_to_normalize])

    elif method == "minmax":
        scaler = MinMaxScaler()
        df_normalized[cols_to_normalize] = scaler.fit_transform(df[cols_to_normalize])

    elif method == "robust":
        scaler = RobustScaler()
        df_normalized[cols_to_normalize] = scaler.fit_transform(df[cols_to_normalize])

    elif method == "log":
        # Add small constant to avoid log(0)
        epsilon = 1e-8
        df_normalized[cols_to_normalize] = np.log(df[cols_to_normalize] + epsilon)

    else:
        raise ValueError(f"Unknown normalization method: {method}")

    return df_normalized


def compare_runs(df, run_ids, metrics=None, plot=True, figsize=(12, 8)):
    """
    Compare specific runs based on selected metrics.

    Parameters:
    -----------
    df : pandas.DataFrame
        DataFrame containing all runs
    run_ids : list
        List of run IDs to compare
    metrics : list or None
        List of metrics to compare. If None, uses a default set
    plot : bool
        Whether to generate comparison plots
    figsize : tuple
        Figure size for plots

    Returns:
    --------
    pandas.DataFrame
        DataFrame with selected metrics for the specified runs
    """
    # Filter dataframe to include only the specified runs
    runs_df = df[df["run_id"].isin(run_ids)]

    if len(runs_df) == 0:
        logger.error(f"No runs found with IDs: {run_ids}")
        return None

    # Default metrics if none specified
    if metrics is None:
        # First try to find the columns with expected names
        available_metrics = []
        metric_candidates = [
            (
                "summary.cumulative.payoff_mean",
                "summary.cumulative_payoff_mean",
                "cumulative_payoff_mean",
            ),
            (
                "summary.final.payoff_mean",
                "summary.final_payoff_mean",
                "final_payoff_mean",
            ),
            (
                "summary.cumulative.unique_items",
                "summary.cumulative_unique_items",
                "cumulative_unique_items",
            ),
            (
                "disaster.average_impact_rmsd",
                "disaster.average_impact_rmsd",
                "average_impact_rmsd",
            ),
            ("disaster.count", "disaster.count", "count"),
        ]

        for metric_group in metric_candidates:
            for metric_name in metric_group:
                if metric_name in runs_df.columns:
                    available_metrics.append(metric_name)
                    break

        # If we have metrics, use them; otherwise just use the first few numeric columns
        if available_metrics:
            metrics = available_metrics
        else:
            # Just use the first 5 numeric columns
            numeric_cols = runs_df.select_dtypes(include=np.number).columns.tolist()
            metrics = numeric_cols[:5] if len(numeric_cols) >= 5 else numeric_cols

    # Check that all metrics exist in the DataFrame
    metrics = [m for m in metrics if m in runs_df.columns]
    if not metrics:
        logger.error("None of the specified metrics found in DataFrame")
        return None

    # Select only the run_id column and the specified metrics
    compare_cols = (
        ["run_id", "param_dir"] if "param_dir" in runs_df.columns else ["run_id"]
    )
    compare_df = runs_df[compare_cols + metrics]

    # If plot is True, generate comparison visualizations
    if plot:
        plt.figure(figsize=figsize)

        # Create a bar chart for each metric
        for i, metric in enumerate(metrics):
            plt.subplot(len(metrics), 1, i + 1)
            sns.barplot(x="run_id", y=metric, data=compare_df)
            plt.title(f"Comparison of {metric}")
            plt.tight_layout()

        plt.show()

    return compare_df


def aggregate_runs(df, group_by=["param_dir"], metrics=None):
    """
    Aggregate runs by specified grouping variables.

    Parameters:
    -----------
    df : pandas.DataFrame
        DataFrame containing all runs
    group_by : list
        List of columns to group by
    metrics : list or None
        List of metrics to aggregate. If None, uses numeric columns

    Returns:
    --------
    pandas.DataFrame
        DataFrame with aggregated metrics
    """
    # Check if grouping columns exist
    for col in group_by:
        if col not in df.columns:
            logger.error(f"Column {col} not found in DataFrame")
            return None

    # Default to numeric columns if metrics not specified
    if metrics is None:
        # Get numeric columns excluding run_id
        metrics = [
            col
            for col in df.select_dtypes(include=np.number).columns
            if col not in ["run_id"] and not col.startswith("time_series")
        ]

    # Make sure all metrics exist in the DataFrame
    metrics = [m for m in metrics if m in df.columns]
    if not metrics:
        logger.error("No valid metrics found in DataFrame")
        return None

    # Subset DataFrame
    agg_df = df[group_by + metrics].copy()

    # Aggregate by mean and std
    result = agg_df.groupby(group_by).agg(["mean", "std", "count"])

    # Flatten multi-index columns
    result.columns = ["_".join(col).strip() for col in result.columns.values]

    return result


def main():
    """
    Example usage of the data loading and analysis functions.
    """
    import argparse

    parser = argparse.ArgumentParser(description="Load and analyze raw simulation data")
    parser.add_argument(
        "--input",
        type=str,
        required=True,
        help="Input directory containing parquet files",
    )
    parser.add_argument(
        "--output", type=str, help="Output directory for figures (optional)"
    )
    parser.add_argument(
        "--compare-runs",
        nargs="+",
        help="Compare specific run IDs (space-separated list)",
    )

    args = parser.parse_args()

    # Load data
    logger.info(f"Loading data from {args.input}")
    data = load_raw_data(args.input)

    if data is None or len(data["data"]) == 0:
        logger.error("Failed to load data or no data found")
        return

    # Display basic information
    logger.info(f"Loaded data for {len(data['param_sets'])} parameter sets")
    logger.info(f"Total runs: {len(data['data'])}")

    # If compare-runs specified, show comparison
    if args.compare_runs:
        logger.info(f"Comparing runs: {args.compare_runs}")
        compare_df = compare_runs(data["data"], args.compare_runs)
        if compare_df is not None:
            print("\nRun Comparison:")
            print(compare_df)

    # Always show parameter aggregation
    agg_df = aggregate_runs(data["data"])
    if agg_df is not None:
        print("\nParameter Set Aggregation (first few columns):")
        # Print just a few columns to avoid overwhelming output
        if len(agg_df.columns) > 6:
            sample_cols = agg_df.columns[:6].tolist()
            print(agg_df[sample_cols])
        else:
            print(agg_df)

    # Create some example visualizations if we have enough data
    if len(data["data"]) > 0:
        try:
            # Add disaster regime categories
            df_with_categories = create_disaster_regime_category(data["data"])

            # First, check if we have the right columns for plotting
            # Get some numeric columns for plotting
            numeric_cols = data["data"].select_dtypes(include=np.number).columns

            if len(numeric_cols) >= 2:
                # Plot distributions of key metrics
                fig, axes = plt.subplots(2, 2, figsize=(14, 10))

                # First plot - histogram of first numeric column
                col1 = numeric_cols[0]
                sns.histplot(data["data"][col1], kde=True, ax=axes[0, 0])
                axes[0, 0].set_title(f"Distribution of {col1}")

                # Second plot - if we have disaster_regime_prob
                if (
                    "disaster_regime_prob" in df_with_categories.columns
                    and col1 in df_with_categories.columns
                ):
                    sns.boxplot(
                        x="disaster_regime_prob",
                        y=col1,
                        data=df_with_categories,
                        ax=axes[0, 1],
                    )
                    axes[0, 1].set_title(f"{col1} by Disaster Probability")
                else:
                    axes[0, 1].set_visible(False)

                # Third plot - if we have at least 2 numeric columns
                if len(numeric_cols) >= 2:
                    col2 = numeric_cols[1]
                    sns.scatterplot(x=col1, y=col2, data=data["data"], ax=axes[1, 0])
                    axes[1, 0].set_title(f"{col1} vs {col2}")
                else:
                    axes[1, 0].set_visible(False)

                # Fourth plot
                group_cols = ["meta.number_of_groups", "number_of_groups"]
                group_col = next(
                    (col for col in group_cols if col in data["data"].columns), None
                )

                if group_col and col1 in data["data"].columns:
                    sns.boxplot(x=group_col, y=col1, data=data["data"], ax=axes[1, 1])
                    axes[1, 1].set_title(f"{col1} by {group_col}")
                else:
                    axes[1, 1].set_visible(False)

                plt.tight_layout()

                # Save figure if output directory is specified
                if args.output:
                    output_dir = Path(args.output)
                    output_dir.mkdir(parents=True, exist_ok=True)
                    fig.savefig(
                        output_dir / "raw_data_analysis.png",
                        dpi=300,
                        bbox_inches="tight",
                    )
                    logger.info(
                        f"Saved figure to {output_dir / 'raw_data_analysis.png'}"
                    )

                plt.show()
        except Exception as e:
            logger.error(f"Error creating visualizations: {e}")


if __name__ == "__main__":
    main()
